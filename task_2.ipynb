{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d40e27a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Iterator\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import pymorphy3\n",
    "from nltk.corpus import stopwords\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3e1e88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    stop_words = set(stopwords.words('russian'))\n",
    "except:\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "morph = pymorphy3.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec602013",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NewsArticle:\n",
    "    \"\"\"Класс для представления новостной статьи\"\"\"\n",
    "    category: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "\n",
    "def load_data(filepath: str) -> List[NewsArticle]:\n",
    "    articles = []\n",
    "    \n",
    "    if filepath.endswith('.gz'):\n",
    "        open_func = gzip.open\n",
    "        mode = 'rt'\n",
    "    else:\n",
    "        open_func = open\n",
    "        mode = 'r'\n",
    "    \n",
    "    with open_func(filepath, mode, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            \n",
    "            parts = line.split('\\t')\n",
    "            if len(parts) >= 3:\n",
    "                category, title, text = parts[0], parts[1], parts[2]\n",
    "                articles.append(NewsArticle(\n",
    "                    category=category.strip(),\n",
    "                    title=title.strip(),\n",
    "                    text=text.strip()\n",
    "                ))\n",
    "    \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a4ee45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str, normalize: bool = True, remove_stopwords: bool = True) -> List[str]:\n",
    "    \"\"\"\n",
    "    Предобработка текста: токенизация, нормализация, удаление стоп-слов\n",
    "    \"\"\"\n",
    "    # Токенизация (разбиение на слова)\n",
    "    words = re.findall(r'\\b[а-яёa-z]+\\b', text.lower())\n",
    "    \n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        if normalize:\n",
    "            parsed = morph.parse(word)[0]\n",
    "            word = parsed.normal_form\n",
    "        \n",
    "        if remove_stopwords and word in stop_words:\n",
    "            continue\n",
    "        \n",
    "        if len(word) < 2:\n",
    "            continue\n",
    "        \n",
    "        processed_words.append(word)\n",
    "    \n",
    "    return processed_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33b40a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(articles: List[NewsArticle], normalize: bool = True, remove_stopwords: bool = True) -> List[List[str]]:\n",
    "    \"\"\"\n",
    "    Подготавливает корпус предложений для обучения Word2Vec\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    for article in articles:\n",
    "        full_text = f\"{article.title} {article.text}\"\n",
    "        tokens = preprocess_text(full_text, normalize, remove_stopwords)\n",
    "        if tokens:\n",
    "            corpus.append(tokens)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bf5c570",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(corpus: List[List[str]], vector_size: int = 100, window: int = 5, \n",
    "                   min_count: int = 2, workers: int = 4) -> Word2Vec:\n",
    "    \"\"\"\n",
    "    Обучает модель Word2Vec на корпусе текстов\n",
    "    \"\"\"\n",
    "    model = Word2Vec(\n",
    "        sentences=corpus,\n",
    "        vector_size=vector_size,\n",
    "        window=window,\n",
    "        min_count=min_count,\n",
    "        workers=workers,\n",
    "        sg=0\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44db3589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_vector_avg(document_tokens: List[str], word2vec_model: Word2Vec) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Базовый метод: преобразует документ в вектор путем усреднения векторов слов\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for token in document_tokens:\n",
    "        if token in word2vec_model.wv:\n",
    "            vectors.append(word2vec_model.wv[token])\n",
    "    \n",
    "    if vectors:\n",
    "        return np.mean(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word2vec_model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4baa3642",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_vector_weighted(document_tokens: List[str], word2vec_model: Word2Vec, \n",
    "                                word_freq: dict = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Альтернативный метод: взвешенное усреднение векторов слов (TF)\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    weights = []\n",
    "    \n",
    "    if word_freq is None:\n",
    "        word_freq = {}\n",
    "        for token in document_tokens:\n",
    "            word_freq[token] = word_freq.get(token, 0) + 1\n",
    "    \n",
    "    total_freq = sum(word_freq.values())\n",
    "    if total_freq == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    \n",
    "    for token in document_tokens:\n",
    "        if token in word2vec_model.wv:\n",
    "            vectors.append(word2vec_model.wv[token])\n",
    "            weights.append(word_freq.get(token, 1) / total_freq)\n",
    "    \n",
    "    if vectors:\n",
    "        weights = np.array(weights)\n",
    "        weights = weights / weights.sum()\n",
    "        return np.average(vectors, axis=0, weights=weights)\n",
    "    else:\n",
    "        return np.zeros(word2vec_model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09a622fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(corpus: List[List[str]], word2vec_model: Word2Vec) -> dict:\n",
    "    \"\"\"\n",
    "    Вычисляет IDF (Inverse Document Frequency) для слов в корпусе\n",
    "    \n",
    "    Args:\n",
    "        corpus: корпус документов\n",
    "        word2vec_model: модель Word2Vec (для фильтрации слов)\n",
    "    \n",
    "    Returns:\n",
    "        Словарь {слово: idf}\n",
    "    \"\"\"\n",
    "    from math import log\n",
    "    \n",
    "    # Подсчитываем количество документов, содержащих каждое слово\n",
    "    doc_freq = {}\n",
    "    total_docs = len(corpus)\n",
    "    \n",
    "    for doc_tokens in corpus:\n",
    "        unique_tokens = set(doc_tokens)\n",
    "        for token in unique_tokens:\n",
    "            if token in word2vec_model.wv:\n",
    "                doc_freq[token] = doc_freq.get(token, 0) + 1\n",
    "    \n",
    "    # Вычисляем IDF\n",
    "    idf = {}\n",
    "    for token, df in doc_freq.items():\n",
    "        # IDF = log(общее_количество_документов / количество_документов_с_словом)\n",
    "        idf[token] = log(total_docs / (df + 1))  # +1 для избежания деления на 0\n",
    "    \n",
    "    return idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a49314a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_vector_tfidf(document_tokens: List[str], word2vec_model: Word2Vec,\n",
    "                             word_freq: dict, idf_dict: dict) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Улучшенный метод: взвешенное усреднение с использованием TF-IDF\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    weights = []\n",
    "    \n",
    "    total_freq = sum(word_freq.values())\n",
    "    if total_freq == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    \n",
    "    for token in document_tokens:\n",
    "        if token in word2vec_model.wv and token in idf_dict:\n",
    "            vectors.append(word2vec_model.wv[token])\n",
    "            tf = word_freq.get(token, 0) / total_freq\n",
    "            idf = idf_dict[token]\n",
    "            weights.append(tf * idf)\n",
    "    \n",
    "    if vectors:\n",
    "        weights = np.array(weights)\n",
    "        weights = weights / (weights.sum() + 1e-10)\n",
    "        return np.average(vectors, axis=0, weights=weights)\n",
    "    else:\n",
    "        return np.zeros(word2vec_model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8dc2a2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_vector_maxpool(document_tokens: List[str], word2vec_model: Word2Vec) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Метод максимального пулинга: берет максимальное значение по каждой размерности\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for token in document_tokens:\n",
    "        if token in word2vec_model.wv:\n",
    "            vectors.append(word2vec_model.wv[token])\n",
    "    \n",
    "    if vectors:\n",
    "        return np.max(vectors, axis=0)\n",
    "    else:\n",
    "        return np.zeros(word2vec_model.vector_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc6ba581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_vector_combined(document_tokens: List[str], word2vec_model: Word2Vec) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Комбинированный метод: конкатенация усреднения и максимального пулинга\n",
    "    \"\"\"\n",
    "    avg_vec = document_to_vector_avg(document_tokens, word2vec_model)\n",
    "    max_vec = document_to_vector_maxpool(document_tokens, word2vec_model)\n",
    "    return np.concatenate([avg_vec, max_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43f1a90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_documents(corpus: List[List[str]], word2vec_model: Word2Vec, \n",
    "                       method: str = 'avg', idf_dict: dict = None) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Преобразует корпус документов в матрицу векторов\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    for doc_tokens in corpus:\n",
    "        if method == 'weighted':\n",
    "            word_freq = {}\n",
    "            for token in doc_tokens:\n",
    "                word_freq[token] = word_freq.get(token, 0) + 1\n",
    "            vec = document_to_vector_weighted(doc_tokens, word2vec_model, word_freq)\n",
    "        elif method == 'tfidf':\n",
    "            if idf_dict is None:\n",
    "                raise ValueError(\"idf_dict required for tfidf method\")\n",
    "            word_freq = {}\n",
    "            for token in doc_tokens:\n",
    "                word_freq[token] = word_freq.get(token, 0) + 1\n",
    "            vec = document_to_vector_tfidf(doc_tokens, word2vec_model, word_freq, idf_dict)\n",
    "        elif method == 'maxpool':\n",
    "            vec = document_to_vector_maxpool(doc_tokens, word2vec_model)\n",
    "        elif method == 'combined':\n",
    "            vec = document_to_vector_combined(doc_tokens, word2vec_model)\n",
    "        else:  # 'avg'\n",
    "            vec = document_to_vector_avg(doc_tokens, word2vec_model)\n",
    "        vectors.append(vec)\n",
    "    \n",
    "    return np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd6bdba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено статей: 10000\n",
      "Пример категорий: {'economics', 'style', 'media', 'forces', 'culture', 'sport'}\n"
     ]
    }
   ],
   "source": [
    "data_path = './data/news.txt.gz'\n",
    "articles = load_data(data_path)\n",
    "print(f\"Загружено статей: {len(articles)}\")\n",
    "print(f\"Пример категорий: {set([a.category for a in articles[:10]])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a73f1803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подготовлено предложений для обучения: 10000\n"
     ]
    }
   ],
   "source": [
    "corpus = prepare_corpus(articles, normalize=True, remove_stopwords=True)\n",
    "print(f\"Подготовлено предложений для обучения: {len(corpus)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a0222bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение Word2Vec модели...\n",
      "Размер словаря модели: 39046\n",
      "Примеры слов в модели: ['год', 'это', 'который', 'россия', 'свой', 'также', 'компания', 'сообщать', 'стать', 'российский']\n"
     ]
    }
   ],
   "source": [
    "print(\"Обучение Word2Vec модели...\")\n",
    "word2vec_model = train_word2vec(corpus, vector_size=100, window=5, min_count=2)\n",
    "print(f\"Размер словаря модели: {len(word2vec_model.wv)}\")\n",
    "print(f\"Примеры слов в модели: {list(word2vec_model.wv.key_to_index.keys())[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1096028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка: 8000 статей\n",
      "Тестовая выборка: 2000 статей\n"
     ]
    }
   ],
   "source": [
    "train_articles, test_articles = train_test_split(\n",
    "    articles, test_size=0.2, random_state=42, stratify=[a.category for a in articles]\n",
    ")\n",
    "print(f\"Обучающая выборка: {len(train_articles)} статей\")\n",
    "print(f\"Тестовая выборка: {len(test_articles)} статей\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "84ab98d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторизация документов (метод: усреднение)...\n",
      "Размерность векторов: (8000, 100)\n"
     ]
    }
   ],
   "source": [
    "train_corpus = prepare_corpus(train_articles, normalize=True, remove_stopwords=True)\n",
    "test_corpus = prepare_corpus(test_articles, normalize=True, remove_stopwords=True)\n",
    "\n",
    "print(\"Векторизация документов (метод: усреднение)...\")\n",
    "X_train = vectorize_documents(train_corpus, word2vec_model, method='avg')\n",
    "X_test = vectorize_documents(test_corpus, word2vec_model, method='avg')\n",
    "y_train = [article.category for article in train_articles]\n",
    "y_test = [article.category for article in test_articles]\n",
    "print(f\"Размерность векторов: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f03a7117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Точность классификации (базовый метод): 0.8065\n",
      "\n",
      "Отчет о классификации:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       0.67      0.19      0.30        72\n",
      "     culture       0.86      0.91      0.89       279\n",
      "   economics       0.76      0.89      0.82       275\n",
      "      forces       0.72      0.76      0.74       154\n",
      "        life       0.72      0.79      0.76       273\n",
      "       media       0.80      0.75      0.77       295\n",
      "     science       0.82      0.79      0.80       286\n",
      "       sport       0.96      0.97      0.96       288\n",
      "       style       0.79      0.69      0.74        39\n",
      "      travel       0.79      0.38      0.52        39\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.79      0.71      0.73      2000\n",
      "weighted avg       0.80      0.81      0.80      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_classifier = LinearSVC(random_state=42, max_iter=1000)\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"\\nТочность классификации (базовый метод): {accuracy:.4f}\")\n",
    "print(\"\\nОтчет о классификации:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "results = {}\n",
    "results['avg'] = accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8cac148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Метод 1: Взвешенное усреднение по TF ---\n",
      "Точность: 0.7855\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Метод 1: Взвешенное усреднение по TF ---\")\n",
    "X_train_weighted = vectorize_documents(train_corpus, word2vec_model, method='weighted')\n",
    "X_test_weighted = vectorize_documents(test_corpus, word2vec_model, method='weighted')\n",
    "svm_weighted = LinearSVC(random_state=42, max_iter=1000)\n",
    "svm_weighted.fit(X_train_weighted, y_train)\n",
    "y_pred_weighted = svm_weighted.predict(X_test_weighted)\n",
    "accuracy_weighted = accuracy_score(y_test, y_pred_weighted)\n",
    "results['weighted'] = accuracy_weighted\n",
    "print(f\"Точность: {accuracy_weighted:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20df5035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Метод 2: TF-IDF взвешивание ---\n",
      "Вычисление IDF...\n",
      "Вычислено IDF для 37362 слов\n",
      "Точность: 0.7870\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Метод 2: TF-IDF взвешивание ---\")\n",
    "print(\"Вычисление IDF...\")\n",
    "idf_dict = compute_idf(train_corpus, word2vec_model)\n",
    "print(f\"Вычислено IDF для {len(idf_dict)} слов\")\n",
    "\n",
    "X_train_tfidf = vectorize_documents(train_corpus, word2vec_model, method='tfidf', idf_dict=idf_dict)\n",
    "X_test_tfidf = vectorize_documents(test_corpus, word2vec_model, method='tfidf', idf_dict=idf_dict)\n",
    "svm_tfidf = LinearSVC(random_state=42, max_iter=1000)\n",
    "svm_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = svm_tfidf.predict(X_test_tfidf)\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "results['tfidf'] = accuracy_tfidf\n",
    "print(f\"Точность: {accuracy_tfidf:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0acb72f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Метод 3: Максимальный пулинг ---\n",
      "Точность: 0.6815\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Метод 3: Максимальный пулинг ---\")\n",
    "X_train_maxpool = vectorize_documents(train_corpus, word2vec_model, method='maxpool')\n",
    "X_test_maxpool = vectorize_documents(test_corpus, word2vec_model, method='maxpool')\n",
    "svm_maxpool = LinearSVC(random_state=42, max_iter=1000)\n",
    "svm_maxpool.fit(X_train_maxpool, y_train)\n",
    "y_pred_maxpool = svm_maxpool.predict(X_test_maxpool)\n",
    "accuracy_maxpool = accuracy_score(y_test, y_pred_maxpool)\n",
    "results['maxpool'] = accuracy_maxpool\n",
    "print(f\"Точность: {accuracy_maxpool:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae703a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Метод 4: Комбинированный (avg + maxpool) ---\n",
      "Точность: 0.8100\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Метод 4: Комбинированный (avg + maxpool) ---\")\n",
    "X_train_combined = vectorize_documents(train_corpus, word2vec_model, method='combined')\n",
    "X_test_combined = vectorize_documents(test_corpus, word2vec_model, method='combined')\n",
    "svm_combined = LinearSVC(random_state=42, max_iter=1000)\n",
    "svm_combined.fit(X_train_combined, y_train)\n",
    "y_pred_combined = svm_combined.predict(X_test_combined)\n",
    "accuracy_combined = accuracy_score(y_test, y_pred_combined)\n",
    "results['combined'] = accuracy_combined\n",
    "print(f\"Точность: {accuracy_combined:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b84b04bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Детальный отчет для лучшего метода: combined ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    business       0.52      0.32      0.40        72\n",
      "     culture       0.87      0.90      0.89       279\n",
      "   economics       0.77      0.85      0.81       275\n",
      "      forces       0.74      0.77      0.76       154\n",
      "        life       0.72      0.81      0.76       273\n",
      "       media       0.82      0.76      0.79       295\n",
      "     science       0.82      0.79      0.80       286\n",
      "       sport       0.95      0.97      0.96       288\n",
      "       style       0.80      0.72      0.76        39\n",
      "      travel       0.68      0.38      0.49        39\n",
      "\n",
      "    accuracy                           0.81      2000\n",
      "   macro avg       0.77      0.73      0.74      2000\n",
      "weighted avg       0.81      0.81      0.81      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_method = max(results, key=results.get)\n",
    "best_accuracy = results[best_method]\n",
    "print(f\"\\n--- Детальный отчет для лучшего метода: {best_method} ---\")\n",
    "if best_method == 'avg':\n",
    "    print(classification_report(y_test, y_pred))\n",
    "elif best_method == 'weighted':\n",
    "    print(classification_report(y_test, y_pred_weighted))\n",
    "elif best_method == 'tfidf':\n",
    "    print(classification_report(y_test, y_pred_tfidf))\n",
    "elif best_method == 'maxpool':\n",
    "    print(classification_report(y_test, y_pred_maxpool))\n",
    "elif best_method == 'combined':\n",
    "    print(classification_report(y_test, y_pred_combined))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79a72c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Сравнение всех методов:\n",
      "============================================================\n",
      "1. combined       : 0.8100 (+0.0035) ← ЛУЧШИЙ\n",
      "2. avg            : 0.8065 (+0.0000)\n",
      "3. tfidf          : 0.7870 (-0.0195)\n",
      "4. weighted       : 0.7855 (-0.0210)\n",
      "5. maxpool        : 0.6815 (-0.1250)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Сравнение всех методов:\")\n",
    "print(\"=\" * 60)\n",
    "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
    "for i, (method, acc) in enumerate(sorted_results, 1):\n",
    "    improvement = acc - accuracy\n",
    "    marker = \" ← ЛУЧШИЙ\" if method == best_method else \"\"\n",
    "    print(f\"{i}. {method:15s}: {acc:.4f} ({improvement:+.4f}){marker}\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_homework",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
