{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация парафразов для тестирования чат-бота\n",
    "\n",
    "Используем предобученные модели Hugging Face для генерации и валидации парафразов.\n",
    "\n",
    "## Содержание\n",
    "1. [Импорты и настройка](#1-импорты-и-настройка)\n",
    "2. [Утилиты для определения типа предложения](#2-утилиты)\n",
    "3. [Генератор парафразов](#3-генератор-парафразов)\n",
    "4. [Валидатор парафразов](#4-валидатор-парафразов)\n",
    "5. [Основной пайплайн](#5-основной-пайплайн)\n",
    "6. [Демонстрация работы](#6-демонстрация)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Импорты и настройка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tizza\\Projects\\nlp_homework\\task3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Используемое устройство: cpu\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Используемое устройство: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Утилиты\n",
    "\n",
    "Функции для определения типа предложения (вопрос, команда, утверждение) и проверки сохранения типа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_sentence_type(text: str) -> str:\n",
    "    \"\"\"Определение типа предложения: question, command, statement.\"\"\"\n",
    "    text = text.strip()\n",
    "    if text.endswith(\"?\"):\n",
    "        return \"question\"\n",
    "        \n",
    "    command_starters = [\n",
    "        \"set\", \"turn\", \"please\", \"remind\", \"call\", \"send\", \"open\", \"close\",\n",
    "        \"play\", \"stop\", \"start\", \"show\", \"tell\", \"find\", \"search\", \"get\",\n",
    "        \"make\", \"create\", \"delete\", \"remove\", \"add\", \"enable\", \"disable\",\n",
    "        \"switch\", \"change\", \"update\", \"check\", \"let\", \"help\", \"give\"\n",
    "    ]\n",
    "    first_word = text.split()[0].lower().rstrip(\",\")\n",
    "    if first_word in command_starters:\n",
    "        return \"command\"\n",
    "    return \"statement\"\n",
    "\n",
    "\n",
    "def preserve_sentence_type(original: str, paraphrase: str) -> bool:\n",
    "    \"\"\"Проверка, что тип предложения сохранён.\"\"\"\n",
    "    orig_type = detect_sentence_type(original)\n",
    "    para_type = detect_sentence_type(paraphrase)\n",
    "    \n",
    "    if orig_type == \"question\":\n",
    "        return para_type == \"question\"\n",
    "    \n",
    "    if orig_type == \"command\":\n",
    "        return para_type != \"question\"\n",
    "    \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование утилит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Определение типа предложений:\n",
      "------------------------------------------------------------\n",
      "[question  ] What's the weather like today?\n",
      "[command   ] Please turn on the lights.\n",
      "[command   ] Set an alarm for 7 AM.\n",
      "[statement ] The weather is nice today.\n",
      "[statement ] After your workout, remember to drink water.\n"
     ]
    }
   ],
   "source": [
    "test_sentences = [\n",
    "    \"What's the weather like today?\",\n",
    "    \"Please turn on the lights.\",\n",
    "    \"Set an alarm for 7 AM.\",\n",
    "    \"The weather is nice today.\",\n",
    "    \"After your workout, remember to drink water.\"\n",
    "]\n",
    "\n",
    "print(\"Определение типа предложений:\")\n",
    "print(\"-\" * 60)\n",
    "for sentence in test_sentences:\n",
    "    sent_type = detect_sentence_type(sentence)\n",
    "    print(f\"[{sent_type:10}] {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Генератор парафразов\n",
    "\n",
    "Класс для генерации парафразов на основе T5-подобных моделей. Поддерживает различные стратегии генерации:\n",
    "- **beam** - beam search\n",
    "- **diverse_beam** - diverse beam search с sampling\n",
    "- **sampling** - top-k sampling\n",
    "- **nucleus** - nucleus (top-p) sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParaphraseGenerator:\n",
    "    \"\"\"\n",
    "    Генератор парафразов на основе T5-подобных моделей.\n",
    "    Поддерживает различные стратегии генерации: beam search, sampling, nucleus sampling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"humarin/chatgpt_paraphraser_on_T5_base\",\n",
    "        device: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Инициализация генератора.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Название модели из Hugging Face Hub\n",
    "            device: Устройство для вычислений ('cuda', 'cpu' или None для автовыбора)\n",
    "        \"\"\"\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Загрузка модели генерации: {model_name}\")\n",
    "        print(f\"Устройство: {self.device}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        print(\"Модель загружена успешно!\")\n",
    "        \n",
    "    def generate(\n",
    "        self,\n",
    "        text: str,\n",
    "        num_paraphrases: int = 10,\n",
    "        strategy: str = \"nucleus\",\n",
    "        max_length: int = 128,\n",
    "        temperature: float = 1.0,\n",
    "        top_p: float = 0.9,\n",
    "        top_k: int = 50\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Генерация парафразов для входного текста.\n",
    "        \n",
    "        Args:\n",
    "            text: Исходный текст для парафразирования\n",
    "            num_paraphrases: Количество парафразов для генерации\n",
    "            strategy: Стратегия генерации ('beam', 'diverse_beam', 'sampling', 'nucleus')\n",
    "            max_length: Максимальная длина генерируемого текста\n",
    "            temperature: Температура для sampling (выше = более разнообразно)\n",
    "            top_p: Параметр для nucleus sampling\n",
    "            top_k: Параметр для top-k sampling\n",
    "            \n",
    "        Returns:\n",
    "            Список сгенерированных парафразов\n",
    "        \"\"\"\n",
    "        sentence_type = detect_sentence_type(text)\n",
    "        \n",
    "        if \"paraphras\" in self.model_name.lower():\n",
    "            input_text = f\"paraphrase: {text}\"\n",
    "        elif \"flan\" in self.model_name.lower():\n",
    "            if sentence_type == \"question\":\n",
    "                input_text = f\"Paraphrase this question while keeping it as a question: {text}\"\n",
    "            elif sentence_type == \"command\":\n",
    "                input_text = f\"Paraphrase this command while keeping it as a command or request: {text}\"\n",
    "            else:\n",
    "                input_text = f\"Paraphrase the following sentence: {text}\"\n",
    "        else:\n",
    "            input_text = f\"Paraphrase the following sentence: {text}\"\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            input_text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=max_length,\n",
    "            truncation=True,\n",
    "            padding=True\n",
    "        ).to(self.device)\n",
    "        \n",
    "        gen_kwargs = {\n",
    "            \"max_length\": max_length,\n",
    "            \"early_stopping\": True,\n",
    "            \"pad_token_id\": self.tokenizer.pad_token_id,\n",
    "        }\n",
    "        \n",
    "        if strategy == \"beam\":\n",
    "            gen_kwargs.update({\n",
    "                \"num_beams\": num_paraphrases,\n",
    "                \"num_return_sequences\": num_paraphrases,\n",
    "            })\n",
    "        elif strategy == \"diverse_beam\":\n",
    "            gen_kwargs.update({\n",
    "                \"num_beams\": num_paraphrases,\n",
    "                \"num_return_sequences\": num_paraphrases,\n",
    "                \"do_sample\": True,\n",
    "                \"temperature\": 1.2,\n",
    "            })\n",
    "        elif strategy == \"sampling\":\n",
    "            gen_kwargs.update({\n",
    "                \"do_sample\": True,\n",
    "                \"num_return_sequences\": num_paraphrases,\n",
    "                \"temperature\": temperature,\n",
    "                \"top_k\": top_k,\n",
    "            })\n",
    "        elif strategy == \"nucleus\":\n",
    "            gen_kwargs.update({\n",
    "                \"do_sample\": True,\n",
    "                \"num_return_sequences\": num_paraphrases,\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": top_p,\n",
    "            })\n",
    "        else:\n",
    "            raise ValueError(f\"Неизвестная стратегия: {strategy}\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(**inputs, **gen_kwargs)\n",
    "        \n",
    "        paraphrases = []\n",
    "        for output in outputs:\n",
    "            decoded = self.tokenizer.decode(output, skip_special_tokens=True)\n",
    "            if decoded.strip() and decoded.strip().lower() != text.strip().lower():\n",
    "                paraphrases.append(decoded.strip())\n",
    "        \n",
    "        seen = set()\n",
    "        unique_paraphrases = []\n",
    "        for p in paraphrases:\n",
    "            p_lower = p.lower()\n",
    "            if p_lower not in seen:\n",
    "                seen.add(p_lower)\n",
    "                unique_paraphrases.append(p)\n",
    "        \n",
    "        return unique_paraphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация генератора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка модели генерации: humarin/chatgpt_paraphraser_on_T5_base\n",
      "Устройство: cpu\n",
      "Модель загружена успешно!\n"
     ]
    }
   ],
   "source": [
    "generator = ParaphraseGenerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование генератора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: What's the weather like today?\n",
      "Тип: question\n",
      "\n",
      "Сгенерированные парафразы:\n",
      "--------------------------------------------------\n",
      "1. What's the weather forecast for today?\n",
      "2. Is there any news on the weather today?\n",
      "3. What's the current weather like?\n"
     ]
    }
   ],
   "source": [
    "test_text = \"What's the weather like today?\"\n",
    "print(f\"Исходный текст: {test_text}\")\n",
    "print(f\"Тип: {detect_sentence_type(test_text)}\")\n",
    "print(\"\\nСгенерированные парафразы:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "paraphrases = generator.generate(test_text, num_paraphrases=5, strategy=\"nucleus\")\n",
    "for i, p in enumerate(paraphrases, 1):\n",
    "    print(f\"{i}. {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение стратегий генерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: Please turn on the lights in the living room.\n",
      "============================================================\n",
      "\n",
      "Стратегия: beam\n",
      "----------------------------------------\n",
      "  1. Please activate the lights fixture in the living room.\n",
      "  2. It is your request that the living room be lit up.\n",
      "  3. We request that you turn on the lights in the lounge.\n",
      "\n",
      "Стратегия: sampling\n",
      "----------------------------------------\n",
      "  1. So, hold on; light up the living room.\n",
      "  2. It would be appreciated if you could switch on the lights in the living room.\n",
      "  3. Kindly enable the lights in the living room.\n",
      "\n",
      "Стратегия: nucleus\n",
      "----------------------------------------\n",
      "  1. It is my responsibility to light up the living room.\n",
      "  2. Please, please turn on the lights in the living room.\n",
      "  3. I request that you put on the lights in the living room.\n"
     ]
    }
   ],
   "source": [
    "test_text = \"Please turn on the lights in the living room.\"\n",
    "strategies = [\"beam\", \"sampling\", \"nucleus\"]\n",
    "\n",
    "print(f\"Исходный текст: {test_text}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for strategy in strategies:\n",
    "    print(f\"\\nСтратегия: {strategy}\")\n",
    "    print(\"-\" * 40)\n",
    "    paraphrases = generator.generate(test_text, num_paraphrases=3, strategy=strategy)\n",
    "    for i, p in enumerate(paraphrases, 1):\n",
    "        print(f\"  {i}. {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Валидатор парафразов\n",
    "\n",
    "Класс для проверки качества парафразов с использованием:\n",
    "- **Embedding similarity** - косинусное сходство эмбеддингов\n",
    "- **Lexical diversity** - лексическое разнообразие (1 - Jaccard similarity)\n",
    "- **NLI (Natural Language Inference)** - проверка логического следования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParaphraseValidator:\n",
    "    \"\"\"\n",
    "    Валидатор качества парафразов.\n",
    "    Использует embedding similarity и NLI для проверки семантической эквивалентности.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "        nli_model: str = \"facebook/bart-large-mnli\",\n",
    "        device: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Инициализация валидатора.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Модель для вычисления эмбеддингов\n",
    "            nli_model: Модель для NLI (Natural Language Inference)\n",
    "            device: Устройство для вычислений\n",
    "        \"\"\"\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        print(f\"Загрузка модели эмбеддингов: {embedding_model}\")\n",
    "        self.embedding_model = SentenceTransformer(embedding_model, device=self.device)\n",
    "        \n",
    "        print(f\"Загрузка NLI модели: {nli_model}\")\n",
    "        self.nli_pipeline = pipeline(\n",
    "            \"zero-shot-classification\",\n",
    "            model=nli_model,\n",
    "            device=0 if self.device == \"cuda\" else -1\n",
    "        )\n",
    "        print(\"Модели валидатора загружены успешно!\")\n",
    "        \n",
    "    def compute_similarity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"\n",
    "        Вычисление косинусного сходства между двумя текстами.\n",
    "        \n",
    "        Args:\n",
    "            text1: Первый текст\n",
    "            text2: Второй текст\n",
    "            \n",
    "        Returns:\n",
    "            Косинусное сходство (от 0 до 1)\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding_model.encode([text1, text2])\n",
    "        similarity = np.dot(embeddings[0], embeddings[1]) / (\n",
    "            np.linalg.norm(embeddings[0]) * np.linalg.norm(embeddings[1])\n",
    "        )\n",
    "        return float(similarity)\n",
    "    \n",
    "    def compute_lexical_diversity(self, text1: str, text2: str) -> float:\n",
    "        \"\"\"\n",
    "        Вычисление лексического разнообразия (1 - Jaccard similarity).\n",
    "        \n",
    "        Args:\n",
    "            text1: Первый текст\n",
    "            text2: Второй текст\n",
    "            \n",
    "        Returns:\n",
    "            Лексическое разнообразие (от 0 до 1, выше = более разнообразно)\n",
    "        \"\"\"\n",
    "        tokens1 = set(text1.lower().split())\n",
    "        tokens2 = set(text2.lower().split())\n",
    "        \n",
    "        if not tokens1 or not tokens2:\n",
    "            return 0.0\n",
    "            \n",
    "        intersection = len(tokens1 & tokens2)\n",
    "        union = len(tokens1 | tokens2)\n",
    "        \n",
    "        jaccard = intersection / union if union > 0 else 0\n",
    "        return 1 - jaccard\n",
    "    \n",
    "    def check_entailment(self, premise: str, hypothesis: str) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Проверка логического следования (entailment) между текстами.\n",
    "        \n",
    "        Args:\n",
    "            premise: Исходное утверждение\n",
    "            hypothesis: Проверяемое утверждение\n",
    "            \n",
    "        Returns:\n",
    "            Кортеж (метка, уверенность)\n",
    "        \"\"\"\n",
    "        result = self.nli_pipeline(\n",
    "            hypothesis,\n",
    "            candidate_labels=[\"entailment\", \"neutral\", \"contradiction\"],\n",
    "            hypothesis_template=\"This example is {}.\"\n",
    "        )\n",
    "        \n",
    "        for label, score in zip(result[\"labels\"], result[\"scores\"]):\n",
    "            if label == \"entailment\":\n",
    "                return (\"entailment\", score)\n",
    "        \n",
    "        return (result[\"labels\"][0], result[\"scores\"][0])\n",
    "    \n",
    "    def validate(\n",
    "        self,\n",
    "        original: str,\n",
    "        candidates: List[str],\n",
    "        sim_min: float = 0.7,\n",
    "        sim_max: float = 0.95,\n",
    "        diversity_min: float = 0.1,\n",
    "        use_nli: bool = True,\n",
    "        nli_threshold: float = 0.5,\n",
    "        check_sentence_type: bool = True\n",
    "    ) -> List[Tuple[str, dict]]:\n",
    "        \"\"\"\n",
    "        Валидация списка кандидатов-парафразов.\n",
    "        \n",
    "        Args:\n",
    "            original: Исходный текст\n",
    "            candidates: Список кандидатов\n",
    "            sim_min: Минимальный порог сходства\n",
    "            sim_max: Максимальный порог сходства (для отсеивания копий)\n",
    "            diversity_min: Минимальное лексическое разнообразие\n",
    "            use_nli: Использовать ли NLI проверку\n",
    "            nli_threshold: Порог для NLI entailment\n",
    "            check_sentence_type: Проверять ли сохранение типа предложения\n",
    "            \n",
    "        Returns:\n",
    "            Список кортежей (парафраз, метрики) для прошедших валидацию\n",
    "        \"\"\"\n",
    "        validated = []\n",
    "        \n",
    "        word_count = len(original.split())\n",
    "        if word_count <= 6:\n",
    "            effective_sim_min = max(0.6, sim_min - 0.1)\n",
    "            effective_nli_threshold = max(0.3, nli_threshold - 0.15)\n",
    "        else:\n",
    "            effective_sim_min = sim_min\n",
    "            effective_nli_threshold = nli_threshold\n",
    "        \n",
    "        for candidate in candidates:\n",
    "            metrics = {}\n",
    "            \n",
    "            if check_sentence_type:\n",
    "                if not preserve_sentence_type(original, candidate):\n",
    "                    metrics[\"rejected\"] = f\"sentence type changed\"\n",
    "                    continue\n",
    "            \n",
    "            similarity = self.compute_similarity(original, candidate)\n",
    "            metrics[\"similarity\"] = similarity\n",
    "            \n",
    "            if similarity < effective_sim_min or similarity > sim_max:\n",
    "                metrics[\"rejected\"] = f\"similarity {similarity:.3f} not in [{effective_sim_min}, {sim_max}]\"\n",
    "                continue\n",
    "            \n",
    "            diversity = self.compute_lexical_diversity(original, candidate)\n",
    "            metrics[\"lexical_diversity\"] = diversity\n",
    "            \n",
    "            if diversity < diversity_min:\n",
    "                metrics[\"rejected\"] = f\"diversity {diversity:.3f} < {diversity_min}\"\n",
    "                continue\n",
    "            \n",
    "            if use_nli:\n",
    "                label_fwd, score_fwd = self.check_entailment(original, candidate)\n",
    "                label_bwd, score_bwd = self.check_entailment(candidate, original)\n",
    "                \n",
    "                metrics[\"nli_forward\"] = {\"label\": label_fwd, \"score\": score_fwd}\n",
    "                metrics[\"nli_backward\"] = {\"label\": label_bwd, \"score\": score_bwd}\n",
    "                \n",
    "                if label_fwd != \"entailment\" or score_fwd < effective_nli_threshold:\n",
    "                    metrics[\"rejected\"] = f\"NLI forward: {label_fwd} ({score_fwd:.3f})\"\n",
    "                    continue\n",
    "                if label_bwd != \"entailment\" or score_bwd < effective_nli_threshold:\n",
    "                    metrics[\"rejected\"] = f\"NLI backward: {label_bwd} ({score_bwd:.3f})\"\n",
    "                    continue\n",
    "            \n",
    "            metrics[\"accepted\"] = True\n",
    "            validated.append((candidate, metrics))\n",
    "        \n",
    "        target_sim = (effective_sim_min + sim_max) / 2\n",
    "        validated.sort(key=lambda x: abs(x[1][\"similarity\"] - target_sim))\n",
    "        \n",
    "        return validated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация валидатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка модели эмбеддингов: sentence-transformers/all-MiniLM-L6-v2\n",
      "Загрузка NLI модели: facebook/bart-large-mnli\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модели валидатора загружены успешно!\n"
     ]
    }
   ],
   "source": [
    "validator = ParaphraseValidator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование отдельных метрик валидатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: What's the weather like today?\n",
      "============================================================\n",
      "\n",
      "Семантическое сходство и лексическое разнообразие:\n",
      "------------------------------------------------------------\n",
      "Similarity: 0.911 | Diversity: 0.571 | How is the weather today?\n",
      "Similarity: 0.744 | Diversity: 0.889 | What is today's weather forecast?\n",
      "Similarity: 0.780 | Diversity: 0.889 | Tell me about the weather.\n",
      "Similarity: 0.158 | Diversity: 0.857 | I like pizza.\n"
     ]
    }
   ],
   "source": [
    "original = \"What's the weather like today?\"\n",
    "paraphrases_to_test = [\n",
    "    \"How is the weather today?\",\n",
    "    \"What is today's weather forecast?\",\n",
    "    \"Tell me about the weather.\",\n",
    "    \"I like pizza.\",  # Совсем другой смысл\n",
    "]\n",
    "\n",
    "print(f\"Исходный текст: {original}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nСемантическое сходство и лексическое разнообразие:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for p in paraphrases_to_test:\n",
    "    sim = validator.compute_similarity(original, p)\n",
    "    div = validator.compute_lexical_diversity(original, p)\n",
    "    print(f\"Similarity: {sim:.3f} | Diversity: {div:.3f} | {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тестирование NLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный текст: Please turn on the lights in the living room.\n",
      "============================================================\n",
      "\n",
      "NLI проверка (entailment в обе стороны):\n",
      "------------------------------------------------------------\n",
      "\n",
      "Switch on the living room lights.\n",
      "  Forward:  entailment (0.550)\n",
      "  Backward: entailment (0.508)\n",
      "\n",
      "Turn off the lights.\n",
      "  Forward:  entailment (0.286)\n",
      "  Backward: entailment (0.508)\n",
      "\n",
      "The lights are on.\n",
      "  Forward:  entailment (0.398)\n",
      "  Backward: entailment (0.508)\n"
     ]
    }
   ],
   "source": [
    "original = \"Please turn on the lights in the living room.\"\n",
    "paraphrases_to_test = [\n",
    "    \"Switch on the living room lights.\",\n",
    "    \"Turn off the lights.\",  # Противоположный смысл\n",
    "    \"The lights are on.\",  # Утверждение, не команда\n",
    "]\n",
    "\n",
    "print(f\"Исходный текст: {original}\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nNLI проверка (entailment в обе стороны):\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for p in paraphrases_to_test:\n",
    "    label_fwd, score_fwd = validator.check_entailment(original, p)\n",
    "    label_bwd, score_bwd = validator.check_entailment(p, original)\n",
    "    print(f\"\\n{p}\")\n",
    "    print(f\"  Forward:  {label_fwd} ({score_fwd:.3f})\")\n",
    "    print(f\"  Backward: {label_bwd} ({score_bwd:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Основной пайплайн\n",
    "\n",
    "Объединённый пайплайн для генерации и валидации парафразов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParaphrasePipeline:\n",
    "    \"\"\"\n",
    "    Объединенный пайплайн для генерации и валидации парафразов.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        generator: Optional[ParaphraseGenerator] = None,\n",
    "        validator: Optional[ParaphraseValidator] = None,\n",
    "        device: Optional[str] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Инициализация пайплайна.\n",
    "        \n",
    "        Args:\n",
    "            generator: Экземпляр генератора (создается по умолчанию если None)\n",
    "            validator: Экземпляр валидатора (создается по умолчанию если None)\n",
    "            device: Устройство для вычислений\n",
    "        \"\"\"\n",
    "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        self.generator = generator or ParaphraseGenerator(device=self.device)\n",
    "        self.validator = validator or ParaphraseValidator(device=self.device)\n",
    "    \n",
    "    def run(\n",
    "        self,\n",
    "        text: str,\n",
    "        num_paraphrases: int = 10,\n",
    "        num_candidates: int = 15,\n",
    "        strategy: str = \"nucleus\",\n",
    "        validate: bool = True,\n",
    "        use_nli: bool = True,\n",
    "        sim_min: float = 0.7,\n",
    "        sim_max: float = 0.95,\n",
    "        verbose: bool = True\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Запуск полного пайплайна генерации парафразов.\n",
    "        \n",
    "        Args:\n",
    "            text: Исходный текст\n",
    "            num_paraphrases: Желаемое количество парафразов на выходе\n",
    "            num_candidates: Количество кандидатов для генерации\n",
    "            strategy: Стратегия генерации\n",
    "            validate: Применять ли валидацию\n",
    "            use_nli: Использовать ли NLI в валидации\n",
    "            sim_min: Минимальный порог сходства\n",
    "            sim_max: Максимальный порог сходства\n",
    "            verbose: Выводить ли подробную информацию\n",
    "            \n",
    "        Returns:\n",
    "            Список валидированных парафразов\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Исходный текст: {text}\")\n",
    "            print(f\"Тип предложения: {detect_sentence_type(text)}\")\n",
    "            print(f\"{'='*60}\")\n",
    "        \n",
    "        word_count = len(text.split())\n",
    "        if word_count <= 6:\n",
    "            effective_candidates = max(num_candidates, 20)\n",
    "        else:\n",
    "            effective_candidates = num_candidates\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nГенерация {effective_candidates} кандидатов (стратегия: {strategy})...\")\n",
    "        \n",
    "        candidates = self.generator.generate(\n",
    "            text,\n",
    "            num_paraphrases=effective_candidates,\n",
    "            strategy=strategy\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Сгенерировано {len(candidates)} уникальных кандидатов\")\n",
    "        \n",
    "        if not validate:\n",
    "            return candidates[:num_paraphrases]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nВалидация кандидатов...\")\n",
    "        \n",
    "        validated = self.validator.validate(\n",
    "            text,\n",
    "            candidates,\n",
    "            sim_min=sim_min,\n",
    "            sim_max=sim_max,\n",
    "            use_nli=use_nli\n",
    "        )\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Прошло валидацию: {len(validated)} из {len(candidates)}\")\n",
    "            \n",
    "            if validated:\n",
    "                print(f\"\\nРезультаты:\")\n",
    "                for i, (paraphrase, metrics) in enumerate(validated[:num_paraphrases], 1):\n",
    "                    sim = metrics.get(\"similarity\", 0)\n",
    "                    div = metrics.get(\"lexical_diversity\", 0)\n",
    "                    print(f\"  {i}. {paraphrase}\")\n",
    "                    print(f\"     [similarity: {sim:.3f}, diversity: {div:.3f}]\")\n",
    "        \n",
    "        return [p for p, _ in validated[:num_paraphrases]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание пайплайна с уже загруженными моделями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Пайплайн готов к работе!\n"
     ]
    }
   ],
   "source": [
    "pipe = ParaphrasePipeline(generator=generator, validator=validator)\n",
    "print(\"Пайплайн готов к работе!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Демонстрация\n",
    "\n",
    "Запустим полный пайплайн на примерах разных типов предложений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 1: Длинное утверждение/совет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Исходный текст: After your workout, remember to focus on maintaining a good water balance.\n",
      "Тип предложения: statement\n",
      "============================================================\n",
      "\n",
      "Генерация 15 кандидатов (стратегия: nucleus)...\n",
      "Сгенерировано 15 уникальных кандидатов\n",
      "\n",
      "Валидация кандидатов...\n",
      "Прошло валидацию: 10 из 15\n",
      "\n",
      "Результаты:\n",
      "  1. Keep a suitable water intake after completing your exercise regimen.\n",
      "     [similarity: 0.808, diversity: 0.778]\n",
      "  2. Keep your water consumption level high after a workout.\n",
      "     [similarity: 0.874, diversity: 0.765]\n",
      "  3. Remember to maintain a healthy water balance after your exercise session.\n",
      "     [similarity: 0.877, diversity: 0.647]\n",
      "  4. Keep your water consumption levels within the healthy range after exercising.\n",
      "     [similarity: 0.762, diversity: 0.850]\n",
      "  5. Make sure to keep a good water intake after your workout.\n",
      "     [similarity: 0.896, diversity: 0.647]\n"
     ]
    }
   ],
   "source": [
    "text = \"After your workout, remember to focus on maintaining a good water balance.\"\n",
    "paraphrases = pipe.run(text, num_paraphrases=5, num_candidates=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 2: Команда (включить свет)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Исходный текст: Please turn on the lights in the living room.\n",
      "Тип предложения: command\n",
      "============================================================\n",
      "\n",
      "Генерация 15 кандидатов (стратегия: nucleus)...\n",
      "Сгенерировано 14 уникальных кандидатов\n",
      "\n",
      "Валидация кандидатов...\n",
      "Прошло валидацию: 2 из 14\n",
      "\n",
      "Результаты:\n",
      "  1. Please activate the lights fixture in the living room.\n",
      "     [similarity: 0.847, diversity: 0.400]\n",
      "  2. It is my responsibility to light up the living room.\n",
      "     [similarity: 0.710, diversity: 0.800]\n"
     ]
    }
   ],
   "source": [
    "text = \"Please turn on the lights in the living room.\"\n",
    "paraphrases = pipe.run(text, num_paraphrases=5, num_candidates=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 3: Команда (установить будильник)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Исходный текст: Set an alarm for 7 AM tomorrow.\n",
      "Тип предложения: command\n",
      "============================================================\n",
      "\n",
      "Генерация 15 кандидатов (стратегия: nucleus)...\n",
      "Сгенерировано 8 уникальных кандидатов\n",
      "\n",
      "Валидация кандидатов...\n",
      "Прошло валидацию: 0 из 8\n"
     ]
    }
   ],
   "source": [
    "text = \"Set an alarm for 7 AM tomorrow.\"\n",
    "paraphrases = pipe.run(text, num_paraphrases=5, num_candidates=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 4: Команда (напоминание)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Исходный текст: Remind me to call mom at 5 PM.\n",
      "Тип предложения: command\n",
      "============================================================\n",
      "\n",
      "Генерация 15 кандидатов (стратегия: nucleus)...\n",
      "Сгенерировано 15 уникальных кандидатов\n",
      "\n",
      "Валидация кандидатов...\n",
      "Прошло валидацию: 12 из 15\n",
      "\n",
      "Результаты:\n",
      "  1. Don't forget to contact mom at 5 PM.\n",
      "     [similarity: 0.813, diversity: 0.545]\n",
      "  2. At 5 PM, please make a call to my mother.\n",
      "     [similarity: 0.838, diversity: 0.714]\n",
      "  3. Please remind me to reach out to my mom at 5 PM.\n",
      "     [similarity: 0.852, diversity: 0.417]\n",
      "  4. At 5 PM, please reach out to mom and tell her.\n",
      "     [similarity: 0.785, diversity: 0.733]\n",
      "  5. Call mom at 5 PM\n",
      "     [similarity: 0.867, diversity: 0.556]\n"
     ]
    }
   ],
   "source": [
    "text = \"Remind me to call mom at 5 PM.\"\n",
    "paraphrases = pipe.run(text, num_paraphrases=5, num_candidates=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 5: Вопрос о погоде"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Исходный текст: What's the weather like today?\n",
      "Тип предложения: question\n",
      "============================================================\n",
      "\n",
      "Генерация 20 кандидатов (стратегия: nucleus)...\n",
      "Сгенерировано 12 уникальных кандидатов\n",
      "\n",
      "Валидация кандидатов...\n",
      "Прошло валидацию: 8 из 12\n",
      "\n",
      "Результаты:\n",
      "  1. What's the weather forecast for today?\n",
      "     [similarity: 0.778, diversity: 0.429]\n",
      "  2. Is there any news about the weather today?\n",
      "     [similarity: 0.737, diversity: 0.700]\n",
      "  3. Can you tell me what's happening in the weather today?\n",
      "     [similarity: 0.830, diversity: 0.636]\n",
      "  4. Can you tell me about the weather today?\n",
      "     [similarity: 0.863, diversity: 0.700]\n",
      "  5. What's the current weather like?\n",
      "     [similarity: 0.895, diversity: 0.571]\n"
     ]
    }
   ],
   "source": [
    "text = \"What's the weather like today?\"\n",
    "paraphrases = pipe.run(text, num_paraphrases=5, num_candidates=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пример 6: Вопрос о маршруте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Исходный текст: How do I get to the nearest coffee shop?\n",
      "Тип предложения: question\n",
      "============================================================\n",
      "\n",
      "Генерация 15 кандидатов (стратегия: nucleus)...\n",
      "Сгенерировано 13 уникальных кандидатов\n",
      "\n",
      "Валидация кандидатов...\n",
      "Прошло валидацию: 11 из 13\n",
      "\n",
      "Результаты:\n",
      "  1. What is the best way to locate a nearby coffee shop?\n",
      "     [similarity: 0.852, diversity: 0.750]\n",
      "  2. Where is the closest coffee shop to my house?\n",
      "     [similarity: 0.859, diversity: 0.800]\n",
      "  3. What is the closest coffee shop to me?\n",
      "     [similarity: 0.860, diversity: 0.786]\n",
      "  4. What is the direction to get to the closest coffee shop?\n",
      "     [similarity: 0.880, diversity: 0.615]\n",
      "  5. How can I locate the closest coffee shop?\n",
      "     [similarity: 0.882, diversity: 0.583]\n"
     ]
    }
   ],
   "source": [
    "text = \"How do I get to the nearest coffee shop?\"\n",
    "paraphrases = pipe.run(text, num_paraphrases=5, num_candidates=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Эксперименты\n",
    "\n",
    "Здесь вы можете попробовать свои примеры и настройки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Исходный текст: Your text here\n",
      "Тип предложения: statement\n",
      "============================================================\n",
      "\n",
      "Генерация 20 кандидатов (стратегия: nucleus)...\n",
      "Сгенерировано 19 уникальных кандидатов\n",
      "\n",
      "Валидация кандидатов...\n",
      "Прошло валидацию: 0 из 19\n"
     ]
    }
   ],
   "source": [
    "my_text = \"Your text here\"\n",
    "\n",
    "num_paraphrases = 5      # Сколько парафразов вернуть\n",
    "num_candidates = 15      # Сколько кандидатов сгенерировать\n",
    "strategy = \"nucleus\"     # beam, diverse_beam, sampling, nucleus\n",
    "use_nli = True           # Использовать NLI проверку\n",
    "sim_min = 0.7            # Минимальное сходство\n",
    "sim_max = 0.95           # Максимальное сходство\n",
    "\n",
    "paraphrases = pipe.run(\n",
    "    my_text,\n",
    "    num_paraphrases=num_paraphrases,\n",
    "    num_candidates=num_candidates,\n",
    "    strategy=strategy,\n",
    "    use_nli=use_nli,\n",
    "    sim_min=sim_min,\n",
    "    sim_max=sim_max,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация без валидации (быстрее)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Исходный текст: What's the weather like today?\n",
      "Тип предложения: question\n",
      "============================================================\n",
      "\n",
      "Генерация 20 кандидатов (стратегия: nucleus)...\n",
      "Сгенерировано 14 уникальных кандидатов\n"
     ]
    }
   ],
   "source": [
    "text = \"What's the weather like today?\"\n",
    "paraphrases = pipe.run(\n",
    "    text,\n",
    "    num_paraphrases=10,\n",
    "    validate=False,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация без NLI (компромисс скорость/качество)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Исходный текст: Please turn on the lights in the living room.\n",
      "Тип предложения: command\n",
      "============================================================\n",
      "\n",
      "Генерация 15 кандидатов (стратегия: nucleus)...\n",
      "Сгенерировано 15 уникальных кандидатов\n",
      "\n",
      "Валидация кандидатов...\n",
      "Прошло валидацию: 10 из 15\n",
      "\n",
      "Результаты:\n",
      "  1. I implore you to switch on the lights in the living room.\n",
      "     [similarity: 0.827, diversity: 0.538]\n",
      "  2. We request that you switch on the lights in the living room.\n",
      "     [similarity: 0.823, diversity: 0.538]\n",
      "  3. Please activate the lights fixture in the living room.\n",
      "     [similarity: 0.847, diversity: 0.400]\n",
      "  4. Please use the lights in the living room.\n",
      "     [similarity: 0.863, diversity: 0.333]\n",
      "  5. It is your request that the lights in the living room be on.\n",
      "     [similarity: 0.778, diversity: 0.750]\n"
     ]
    }
   ],
   "source": [
    "text = \"Please turn on the lights in the living room.\"\n",
    "paraphrases = pipe.run(\n",
    "    text,\n",
    "    num_paraphrases=5,\n",
    "    use_nli=False,\n",
    "    verbose=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "task3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
